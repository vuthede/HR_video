{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-865c7179a5cf>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-865c7179a5cf>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(a . * -1)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import basename\n",
    "import audiosegment\n",
    "from multiprocessing import Pool\n",
    "modulePath = 'ChristiansPythonLibrary/src' \n",
    "import sys\n",
    "import numpy\n",
    "sys.path.append(modulePath)\n",
    "import generalUtility\n",
    "import dspUtil\n",
    "import matplotlibUtil\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#Constant\n",
    "EMOTION_ANNOTATORS = {'anger': 0, 'happiness' : 1, 'sadness' : 2, 'neutral' : 3, 'frustration' : 4, 'excited': 5,\n",
    "           'fear' : 6,'surprise' : 7,'disgust' : 8, 'other' : 9}\n",
    "\n",
    "EMOTION = {'ang': 0, 'hap' : 1, 'sad' : 2, 'neu' : 3, 'fru' : 4, 'exc': 5,\n",
    "           'fea' : 6,'sur' : 7,'dis' : 8, 'oth' : 9, 'xxx':10}\n",
    "\n",
    "METHOD = {'audio_feature':0, 'LSTM':1, 'LSTM_KERAS':2}\n",
    "\n",
    "#Method for classification\n",
    "method = METHOD['LSTM_KERAS']\n",
    "\n",
    "#If data is processed and saved into files, just reload, dont need to re-process\n",
    "isRawDataProcessed = True\n",
    "\n",
    "#Development mode. Only run with small data.\n",
    "dev = False\n",
    "\n",
    "\n",
    "\n",
    "#Define class\n",
    "class Input:\n",
    "    ##spectral, prosody, erergy are dict type\n",
    "    def __init__(self, spectral, prosody, energy, spectrogram):\n",
    "        self.spectral = spectral\n",
    "        self.prosody = prosody\n",
    "        self.energy = energy\n",
    "        self.spectrogram = spectrogram\n",
    "        \n",
    "    def print(self):\n",
    "        print(\"spectral  features: \", spectral)\n",
    "        print(\"prosody features: \", prosody)\n",
    "        print(\"energy: \", energy)\n",
    "        print(\"spectrogram: \", spectrogram)\n",
    "        \n",
    "    def input2Vec(self, onlySpectrogram):\n",
    "        if (onlySpectrogram ==  False):\n",
    "            features = []\n",
    "            s = list(self.spectral.values())\n",
    "            p = list(self.prosody.values())\n",
    "            e = list(self.energy.values())\n",
    "            [features.extend(x) for x in [s, p, e]]\n",
    "            return features\n",
    "        else :\n",
    "            return self.spectrogram\n",
    "    \n",
    "class Output:\n",
    "    def __init__(self, duration, code, category_origin, category_evaluation, attribute):\n",
    "        self.duration = duration\n",
    "        self.code = code\n",
    "        self.category_origin = category_origin\n",
    "        self.category_evaluation = category_evaluation\n",
    "        self.attribute = attribute\n",
    "        \n",
    "     \n",
    "    def print(self):\n",
    "        print(\"duration: \", self.duration)\n",
    "        print(\"code: \", self.code)\n",
    "        print(\"category_origin: \", self.category_origin)\n",
    "        print(\"category_evaluation: \", self.category_evaluation)\n",
    "        print(\"attribute: \", self.attribute)\n",
    "        \n",
    "    def output2Vec(self):\n",
    "        emotion = EMOTION[self.category_origin]\n",
    "        return emotion\n",
    "    \n",
    "    \n",
    "    \n",
    "#Functions for get features from audio file\n",
    "def amp2Db(samples):\n",
    "    dbs = []\n",
    "    for  x in samples:\n",
    "        if x < 0:\n",
    "            v = - dspUtil.rmsToDb(np.abs(x))\n",
    "        elif x == 0:\n",
    "            v = 0\n",
    "        else :\n",
    "            v = dspUtil.rmsToDb(np.abs(x))\n",
    "        dbs.append(v)\n",
    "    return dbs\n",
    "\n",
    "def getF0Features(file):\n",
    "    features = {}\n",
    "    sound = audiosegment.from_file(file)\n",
    "    voiced = sound.filter_silence(duration_s=0.2)\n",
    "    frame_rate = sound.frame_rate\n",
    "    frames = sound.dice(0.032)\n",
    "\n",
    "    f0s = []\n",
    "    for f in frames:\n",
    "        f0 = dspUtil.calculateF0once(amp2Db(f.get_array_of_samples()), frame_rate)\n",
    "        if(f0 != 0):\n",
    "            f0s.append(f0)\n",
    "    \n",
    "    features['f0_min'] = np.min(f0s)\n",
    "    features['f0_max'] = np.max(f0s)\n",
    "    features['f0_range'] = np.max(f0s) - np.min(f0s)\n",
    "    features['f0_mean'] = np.mean(f0s)\n",
    "    features['f0_median'] = np.median(f0s)\n",
    "    features['f0_25th'] = np.percentile(f0s, 25)\n",
    "    features['f0_75th'] = np.percentile(f0s, 75)\n",
    "    features['f0_std'] = np.std(f0s)\n",
    "    \n",
    "  \n",
    "    return features\n",
    "\n",
    "def getEnergyFeatures(file):\n",
    "    features = {}\n",
    "    sound = audiosegment.from_file(file)\n",
    "    voiced = sound.filter_silence(duration_s=0.2)\n",
    "    samples = voiced.get_array_of_samples()\n",
    "    frame_rate = sound.frame_rate\n",
    "    frames = sound.dice(0.032)\n",
    "    \n",
    "    e = []\n",
    "    for f in frames:\n",
    "        e.append(np.abs(f.max_dBFS))\n",
    "    \n",
    "    \n",
    "    features['energy_min'] = np.min(e)\n",
    "    features['energy_max'] = np.max(e)\n",
    "    features['energy_range'] = np.max(e) - np.min(e)\n",
    "    features['energy_mean'] = np.mean(e)\n",
    "    features['energy_median'] = np.median(e)\n",
    "    features['energy_25th'] = np.percentile(e, 25)\n",
    "    features['energy_75th'] = np.percentile(e, 75)\n",
    "    features['energy_std'] = np.std(e)   \n",
    "\n",
    "    return features\n",
    "    \n",
    "def audio2Features(file):\n",
    "    spectral = {}\n",
    "    prosody = {}\n",
    "    energy = {}\n",
    "    try:\n",
    "        prosody = getF0Features(file)\n",
    "        energy = getEnergyFeatures(file)\n",
    "        \n",
    "        y, sr = librosa.load(file)\n",
    "        spectrogram = librosa.stft(y)\n",
    "        spectrogram = np.abs(spectrogram)\n",
    "        #To be continued....\n",
    "    \n",
    "        return Input(spectral, prosody, energy, spectrogram)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "#Function for getting input vector and corresponding output      \n",
    "def parallel_task(d0, d1):\n",
    "    print(\"task...\")\n",
    "    # Each input diectory contains many file\n",
    "    # This fucntion will walk through all valid 'wav'files in this directory and get features like engergy, frequency...\n",
    "    def parseInput(dir):\n",
    "        dicts = {} \n",
    "        for f in os.listdir(dir):\n",
    "            if not f.startswith(\".\") and os.path.splitext(f)[1] == \".wav\":\n",
    "                dicts[os.path.splitext(f)[0]] = audio2Features(dir + \"/\" + f)\n",
    "\n",
    "\n",
    "        return dicts\n",
    "    \n",
    "    # Each output file contains label of many diffrent 'wav' file.\n",
    "    # This function will parse content of text file using 'regrex'. Then turn it into label\n",
    "    def parseOutput(file):\n",
    "        dict_namefile_output = {}\n",
    "        # Open file to get all contents excepts the first line.\n",
    "        f = open(file, 'r')\n",
    "        content = \"\"\n",
    "        index = 0\n",
    "        for line in f:\n",
    "            index = index + 1\n",
    "            if index == 1:\n",
    "                continue\n",
    "            content  = content + line\n",
    "\n",
    "        # Find all matched patterns in the content\n",
    "        ps = re.findall(r'\\[.*?\\)\\n\\n', content, re.DOTALL)\n",
    "\n",
    "        # Parse each matched pattern into  'Output' object\n",
    "        try:\n",
    "            for p in ps:\n",
    "                ls = p.split(\"\\n\")\n",
    "                ls = list(filter(lambda x: len(x) > 0 ,ls))\n",
    "\n",
    "                # Split elements of the first line which looks like : \n",
    "                # [147.0300 - 151.7101]\tSes01F_impro02_M012\tneu\t[2.5000, 2.0000, 2.0000]\n",
    "                ele_line0 = re.search(r'(\\[.*?\\])(\\s)(.*?)(\\s)(.*?)(\\s)(\\[.*?\\])', ls[0]).groups()\n",
    "\n",
    "                # Split time components which looks like:\n",
    "                # [147.0300 - 151.7101]\n",
    "                time_dur = ele_line0[0]\n",
    "                ele_time_dur = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", time_dur)\n",
    "                ele_time_dur = [float(x) for x in ele_time_dur]\n",
    "\n",
    "                # Get code and category_origin which looks like:\n",
    "                # Code: Ses01F_impro02_M012\n",
    "                # Category_origin: neu\n",
    "                code = ele_line0[2]\n",
    "                category_origin = ele_line0[4]\n",
    "\n",
    "                # Split attribute components which looks like:\n",
    "                # [2.5000, 2.0000, 2.0000]\n",
    "                attribute = ele_line0[6]\n",
    "                ele_attribute = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", attribute)\n",
    "                ele_attribute = [float(x) for x in ele_attribute]\n",
    "\n",
    "                # Get categorial_evaluation:\n",
    "                lines_categorical = list(filter(lambda x : x[0] == 'C', ls))\n",
    "                rex = re.compile(r'C.*?:(\\s)(.*?)(\\s)\\(.*?\\)')\n",
    "\n",
    "                category_evaluation = []\n",
    "                for l in lines_categorical:\n",
    "                    elements = rex.search(l).groups()\n",
    "                    cat = elements[1]\n",
    "                    cat = cat.split(\";\")\n",
    "                    cat = map(lambda x: x.lstrip(), cat)\n",
    "                    cat = list(filter(lambda x: len(x)>0, cat))\n",
    "                    category_evaluation.extend(cat)\n",
    "\n",
    "\n",
    "                # Make list distinct\n",
    "                category_evaluation = list(set(category_evaluation))\n",
    "                \n",
    "                \n",
    "\n",
    "                # Make dict {name_file : parsed_output}\n",
    "                dict_namefile_output[code] = Output(ele_time_dur, code, category_origin, category_evaluation, ele_attribute)\n",
    "            return dict_namefile_output\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    ### Parse input and output files and get input and output as vector\n",
    "    dicts_in = parseInput(d0)\n",
    "    dicts_out = parseOutput(d1)\n",
    "    in_out = []\n",
    "    \n",
    "    keys = list(dicts_in.keys())\n",
    "    for key in keys:\n",
    "        if(dicts_out[key].category_origin != 'xxx'):\n",
    "            if (method == METHOD['LSTM']):\n",
    "                in_out.append((dicts_in[key].input2Vec(onlySpectrogram=True), dicts_out[key].output2Vec()))\n",
    "            else:\n",
    "                in_out.append((dicts_in[key].input2Vec(onlySpectrogram=False), dicts_out[key].output2Vec()))\n",
    "    return in_out\n",
    "    \n",
    "    \n",
    "def createInput_Output():\n",
    "    ### Get directories of input and output\n",
    "    DATA_DIR = \"IEMOCAP_full_release\"\n",
    "    NUM_SESSION = 5\n",
    "    input_output = []\n",
    "    for i in range (1, NUM_SESSION + 1):\n",
    "        name_session = \"Session\" + str(i)\n",
    "        root_dir_of_wav = DATA_DIR + \"/\" + name_session + \"/sentences\" + \"/wav\"\n",
    "        root_dir_of_labels = DATA_DIR + \"/\" + name_session + \"/dialog\" + \"/EmoEvaluation\"\n",
    "\n",
    "        for x in os.walk(root_dir_of_wav):\n",
    "            if(x[0] == root_dir_of_wav):\n",
    "                dirs_of_wav = x[1]\n",
    "                index = -1\n",
    "            else:\n",
    "                index = index + 1\n",
    "                input_output.append((x[0], root_dir_of_labels + \"/\" + dirs_of_wav[index] + \".txt\"))\n",
    "                \n",
    "    \n",
    "    ds = input_output\n",
    "    in_out = []\n",
    "    input = []\n",
    "    out = []\n",
    "    \n",
    "    # Multi processing\n",
    "    with Pool(processes=8) as pool:\n",
    "         in_out = pool.starmap(parallel_task, ds)\n",
    "   \n",
    "    r = []\n",
    "    for e in in_out:\n",
    "        r = r + e\n",
    "    \n",
    "    input = [x[0] for x in r]\n",
    "    out = [x[1] for x in r]\n",
    "    print(\"Finished creating input output into txt file\")\n",
    "    return (input, out)\n",
    " \n",
    "\n",
    "\n",
    "#If have not processed data yet then process, otherwise loading data from file.\n",
    "if isRawDataProcessed == False:\n",
    "\n",
    "    ##Get input, normalize input, get output\n",
    "    input, output = createInput_Output()\n",
    "    output = np.array(output)\n",
    "    \n",
    "    if(method == METHOD['audio_feature']):\n",
    "        input = np.array(input)\n",
    "        input = input / input.max(axis=0)\n",
    "        filehandlerInput = open('processed-data/input0.obj', 'wb')\n",
    "        filehandlerOutput = open('processed-data/output0.obj', 'wb')\n",
    "    elif(method == METHOD['LSTM']):\n",
    "        # After this operator, each sample will be a 2-D array, Each row includes magnitude energy values in range of frquencies\n",
    "        # Rows will have the same length in all samples.\n",
    "        # Each sample will have different number of rows beacause their difference of length in seconds \n",
    "        #input = [list(map(list, zip(*i))) for i in input]\n",
    "        filehandlerInput = open('processed-data/input1.obj', 'wb')\n",
    "        filehandlerOutput = open('processed-data/output1.obj', 'wb')\n",
    "\n",
    "        \n",
    "    pickle.dump(input, filehandlerInput)\n",
    "    pickle.dump(output, filehandlerOutput)\n",
    "    print(\"Finish write processed data (input, output) to file!!!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Reshape, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import timeit\n",
    "from sklearn.utils import class_weight\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "dev = True\n",
    "size_batch = 10\n",
    "window_size = 25\n",
    "num_class = 3\n",
    "if method == METHOD['LSTM_KERAS']: \n",
    "    ##Loading  data from files\n",
    "    filehandlerInput = open('processed-data/input1.obj', 'rb')\n",
    "    filehandlerOutput = open('processed-data/output1.obj', 'rb')\n",
    "    input = pickle.load(filehandlerInput)\n",
    "    output = pickle.load(filehandlerOutput)\n",
    "    \n",
    "    #if in development mode, just use the small data!\n",
    "    if (dev):\n",
    "        data_size = 300\n",
    "    else:\n",
    "        data_size = len(input)\n",
    "    \n",
    "    print(\"\\nThe number of samples \", data_size)\n",
    "    \n",
    "    # Input : (size of samples, ?, f_range = 1025)\n",
    "    input = [list(map(list, zip(*i))) for i in input[0:data_size]]\n",
    "    \n",
    "\n",
    "    #Normalize input\n",
    "    max_val = -1\n",
    "    for i in input:\n",
    "        b = [max(x) for x in i]\n",
    "        c = max(b)\n",
    "        if c >  max_val:\n",
    "            max_val = c\n",
    "\n",
    "    print(\"\\nThis is max_value of input: \", max_val)\n",
    "    \n",
    "    #Normalize\n",
    "    for i in range(0, len(input)):\n",
    "        for j in range(0, len(input[i])):\n",
    "            input[i][j] = [x / max_val for x in input[i][j]]\n",
    "            \n",
    "    \n",
    "    print(\"\\nFinished normalize input.\")\n",
    "\n",
    "    #Widen the width of the data with the window_size\n",
    "    for j in range(0,len(input)):\n",
    "        t = []\n",
    "        for i in range(0, len(input[j]), window_size):\n",
    "            flat_list = [item for sublist in input[j][i:i+window_size] for item in sublist]\n",
    "            t.append(flat_list)\n",
    "        input[j] = t\n",
    "        input[j] = t[0:len(t) - 1]\n",
    "        \n",
    "    print(\"\\nFinished widen window_size!\")\n",
    "    \n",
    "    \n",
    "    output = output[0:data_size]\n",
    "    \n",
    "    #Cluster output\n",
    "    for i in range (0, len(output)):\n",
    "        if (output[i] % 2 == 0):\n",
    "            output[i] = 0\n",
    "        elif (output[i] == 1 or output[i] == 5):\n",
    "            output[i] = -1\n",
    "        elif (output[i] == 3):\n",
    "            output[i] = -2\n",
    "    \n",
    "    output = [x * -1 for x in output]\n",
    "    print(\"Finish clustering emotion\")\n",
    "    \n",
    "    # Get original quantiry of each label\n",
    "    y = np.bincount(output)\n",
    "    ii = np.nonzero(y)[0]\n",
    "    a = list(zip(ii,y[ii]))\n",
    "    #Get quantity of the largest sample.\n",
    "    max_sample = max(a, key=lambda x:x[1])[1]\n",
    "    #print(\"max_sample: \",max_sample)\n",
    "    \n",
    "    print(\"EMOTION_ANNOTATE: \", EMOTION_ANNOTATORS)\n",
    "    print(\"\\nThe quantity of each label: \", a, \"\\n\")\n",
    "    \n",
    "    #Get the class_weight of each class\n",
    "    \n",
    "    class_weight = class_weight.compute_class_weight('balanced'\n",
    "                                               ,np.unique(output)\n",
    "                                               ,output)\n",
    "    \n",
    "    #Remove labels that have small quantity.\n",
    "    indices = [] \n",
    "    for i in range(0, len(output)):\n",
    "        if output[i] >= 3:\n",
    "            indices.append(i)\n",
    "    input = np.delete(input, indices, axis = 0)\n",
    "    output = np.delete(output, indices)\n",
    "    print(\"\\nRemoved samples that have the tiny quantity!\")\n",
    "\n",
    "    #shuffle data\n",
    "    c = list(zip(input, output))\n",
    "    random.shuffle(c)\n",
    "    input, output = zip(*c)\n",
    "    \n",
    "    print(\"\\nFinished shuffling all data!\")\n",
    "    \n",
    "    #Group the same values into each bucket\n",
    "    c = list(zip(input, output))\n",
    "    values = list(set(map(lambda x:x[1], c)))\n",
    "    input_groups = [[y[0] for y in c if y[1]==x] for x in values]\n",
    "  \n",
    "\n",
    "    # Split train, test\n",
    "    trainlen = int(0.8 * data_size)\n",
    "    train_in = []\n",
    "    train_out = []\n",
    "    test_in = [] \n",
    "    test_out = []\n",
    "   \n",
    "    for i in range(0, len(values)):\n",
    "        out_label = np.zeros((1,num_class))[0]\n",
    "        out_label[values[i]] = 1\n",
    "\n",
    "        len_group = len(input_groups[i])\n",
    "        ratio_sample_int = int(max_sample / len_group)\n",
    "        ratio_sample_remain = max_sample / len_group - ratio_sample_int\n",
    "        \n",
    "        new_train_in = input_groups[i][0:int(0.8 * len_group)]\n",
    "        \n",
    "        new_train_in_up_sampling = list(np.repeat(new_train_in, ratio_sample_int, axis = 0))\n",
    "        number_addition_sample = int(ratio_sample_remain * len(new_train_in)) - len(new_train_in)\n",
    "        new_train_in_up_sampling = new_train_in_up_sampling + new_train_in[0:number_addition_sample]\n",
    "       \n",
    "        train_in  = train_in + new_train_in_up_sampling\n",
    "        \n",
    "        print(\"Up sampling sample \", i, \"in training set. \", \"The new length is: \", len(new_train_in_up_sampling))\n",
    "        new_test_in = input_groups[i][int(0.8 * len_group) : len_group]\n",
    "        test_in = test_in + new_test_in\n",
    "        train_out = train_out + [out_label] * len(new_train_in_up_sampling)\n",
    "        test_out = test_out + [out_label] * len(new_test_in)\n",
    "\n",
    "    print(\"\\nFinished upsampling data!\")\n",
    "        \n",
    "    # to numpy array    \n",
    "    train_out = np.array(train_out)\n",
    "    test_out = np.array(test_out)\n",
    "\n",
    "\n",
    "    #shuffle data\n",
    "    c = list(zip(train_in, train_out))\n",
    "    random.shuffle(c)\n",
    "    train_in, train_out = zip(*c)\n",
    "    \n",
    "    \n",
    "    print(\"\\nFinished shuffling training set\")\n",
    "    \n",
    "#     # Split train, test\n",
    "#     trainlen = int(0.6*data_size)\n",
    "#     train_in, train_out = input[0:trainlen], output[0:trainlen]\n",
    "#     test_in, test_out = input[trainlen:data_size], output[trainlen:data_size]\n",
    "#     print(\"Finished split data!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_dim = 1025 * window_size\n",
    "    timesteps = None\n",
    "    \n",
    "\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(144, return_sequences=True, input_shape=(None, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(144, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(144))  # return a single vector of dimension 32\n",
    "#     model.add(Reshape((12,12,1), input_shape=(144, )))\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=(1,12,12,1)))\n",
    "   \n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(6))#fully connected\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    def accuracy(model, input, output):\n",
    "        sumTrue = 0\n",
    "        for x,y in zip(input, output):\n",
    "            p = model.predict_classes(np.array([x]))\n",
    "            if (p[0] == np.argmax(y)):\n",
    "                sumTrue = sumTrue + 1\n",
    "        \n",
    "        return sumTrue / len(input)\n",
    "            \n",
    "    \n",
    "    try:\n",
    "        step = 0\n",
    "        for i in range (0,4):\n",
    "            for seq, label in zip(train_in, train_out):\n",
    "                step = step + 1\n",
    "                weight_sample = class_weight[np.argmax(label)]\n",
    "                #sample_weight = np.array([weight_sample])\n",
    "\n",
    "                model.train_on_batch(np.array([seq]), np.array([label]))\n",
    "                if (step == 1 or step % 500 == 0):\n",
    "\n",
    "                    print(\"Trained 500 sample!\")\n",
    "                    print(\"Score train set: \", accuracy(model, train_in, train_out))\n",
    "                    print(\"Score test set; \", accuracy(model, test_in, test_out))\n",
    "    except Exception as e:\n",
    "        print(\"This is error: \", e)\n",
    "\n",
    "    \n",
    "    print(\"\\nAfter training........................................................\")\n",
    "    print(\"Score train set: \", accuracy(model, train_in, train_out))\n",
    "    print(\"Score test set; \", accuracy(model, test_in, test_out))\n",
    "    \n",
    "\n",
    "    ps = []\n",
    "    for x, y in zip (test_in, test_out):\n",
    "        p = model.predict_classes(np.array([x]))\n",
    "#         print(p)\n",
    "        ps.append(p)\n",
    "    r = np.argmax(test_out, axis=1)\n",
    "    #p = model.predict_classes(test_in)\n",
    "   \n",
    "    matrix = confusion_matrix(r, ps)\n",
    "    print(\"\\nConfusion matrix: \\n\", matrix)\n",
    "    sum_colum = np.sum(matrix, axis = 0)\n",
    "   # print(\"\\nsum_column:\", sum_colum)\n",
    "    sum_row = np.sum(matrix, axis = 1)\n",
    "    #print(\"\\nsum_row:\", sum_row)\n",
    "    TP = [matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"\\nTP: \", TP,\"\\n\")   \n",
    "    FP = [sum_colum[i] - matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"FP: \", FP,\"\\n\")\n",
    "    FN = [sum_row[i] - matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"FN: \", FN,\"\\n\")\n",
    "    Presision = [TP[i] /(TP[i] + FP[i])  for i in range(0, len(matrix))]\n",
    "    Recall = [TP[i] /(TP[i] + FN[i])  for i in range(0, len(matrix))]\n",
    "    F1_score = [2 * Presision[i] * Recall[i] /(Presision[i] + Recall[i])  for i in range(0, len(matrix))]\n",
    "    \n",
    "  \n",
    "    \n",
    "    print(\"\\nPrecision: \", Presision,\"\\n\")\n",
    "    print(\"Recall: \", Recall,\"\\n\")\n",
    "    print(\"F1_scrore: \", F1_score, \"\\n\")\n",
    "    \n",
    "        \n",
    "    #Your statements here\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"Time for training and testing: \", stop - start, \"(s)\") \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
