{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Reshape, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import timeit\n",
    "from sklearn.utils import class_weight\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "#Constant\n",
    "EMOTION_ANNOTATORS = {'anger': 0, 'happiness' : 1, 'sadness' : 2, 'neutral' : 3, 'frustration' : 4, 'excited': 5,\n",
    "           'fear' : 6,'surprise' : 7,'disgust' : 8, 'other' : 9}\n",
    "\n",
    "EMOTION = {'ang': 0, 'hap' : 1, 'sad' : 2, 'neu' : 3, 'fru' : 4, 'exc': 5,\n",
    "           'fea' : 6,'sur' : 7,'dis' : 8, 'oth' : 9, 'xxx':10}\n",
    "\n",
    "METHOD = {'audio_feature':0, 'LSTM':1}\n",
    "\n",
    "#Method for classification\n",
    "method = METHOD['LSTM']\n",
    "\n",
    "#If data is processed and saved into files, just reload, dont need to re-process\n",
    "isRawDataProcessed = True\n",
    "\n",
    "#Development mode. Only run with small data.\n",
    "dev = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dev = True\n",
    "size_batch = 10\n",
    "window_size = 25\n",
    "num_class = 6\n",
    "if method == METHOD['LSTM']: \n",
    "    ##Loading  data from files\n",
    "    filehandlerInput = open('processed-data/input.obj', 'rb')\n",
    "    filehandlerOutput = open('processed-data/output.obj', 'rb')\n",
    "    input = pickle.load(filehandlerInput)\n",
    "    output = pickle.load(filehandlerOutput)\n",
    "    \n",
    "    #if in development mode, just use the small data!\n",
    "    if (dev):\n",
    "        data_size = 4000\n",
    "    else:\n",
    "        data_size = len(input)\n",
    "    \n",
    "    print(\"\\nThe number of samples \", data_size)\n",
    "    \n",
    "    # Input : (size of samples, ?, f_range = 1025)\n",
    "    input = [list(map(list, zip(*i))) for i in input[0:data_size]]\n",
    "    \n",
    "\n",
    "    #Normalize input\n",
    "    max_val = -1\n",
    "    for i in input:\n",
    "        b = [max(x) for x in i]\n",
    "        c = max(b)\n",
    "        if c >  max_val:\n",
    "            max_val = c\n",
    "\n",
    "    print(\"\\nThis is max_value of input: \", max_val)\n",
    "    \n",
    "    #Normalize\n",
    "    for i in range(0, len(input)):\n",
    "        for j in range(0, len(input[i])):\n",
    "            input[i][j] = [x / max_val for x in input[i][j]]\n",
    "            \n",
    "    \n",
    "    print(\"\\nFinished normalize input.\")\n",
    "\n",
    "    #Widen the width of the data with the window_size\n",
    "    for j in range(0,len(input)):\n",
    "        t = []\n",
    "        for i in range(0, len(input[j]), window_size):\n",
    "            flat_list = [item for sublist in input[j][i:i+window_size] for item in sublist]\n",
    "            t.append(flat_list)\n",
    "        input[j] = t\n",
    "        input[j] = t[0:len(t) - 1]\n",
    "        \n",
    "    print(\"\\nFinished widen window_size!\")\n",
    "    \n",
    "    \n",
    "    output = output[0:data_size]\n",
    "    \n",
    "    # Get original quantiry of each label\n",
    "    y = np.bincount(output)\n",
    "    ii = np.nonzero(y)[0]\n",
    "    a = list(zip(ii,y[ii]))\n",
    "    #Get quantity of the largest sample.\n",
    "    max_sample = max(a, key=lambda x:x[1])[1]\n",
    "    #print(\"max_sample: \",max_sample)\n",
    "    \n",
    "    print(\"EMOTION_ANNOTATE: \", EMOTION_ANNOTATORS)\n",
    "    print(\"\\nThe quantity of each label: \", a, \"\\n\")\n",
    "    \n",
    "    #Get the class_weight of each class\n",
    "    \n",
    "    class_weight = class_weight.compute_class_weight('balanced'\n",
    "                                               ,np.unique(output)\n",
    "                                               ,output)\n",
    "    \n",
    "    #Remove labels that have small quantity.\n",
    "    indices = [] \n",
    "    for i in range(0, len(output)):\n",
    "        if output[i] >= 6:\n",
    "            indices.append(i)\n",
    "    input = np.delete(input, indices, axis = 0)\n",
    "    output = np.delete(output, indices)\n",
    "    print(\"\\nRemoved samples that have the tiny quantity!\")\n",
    "\n",
    "    #shuffle data\n",
    "    c = list(zip(input, output))\n",
    "    random.shuffle(c)\n",
    "    input, output = zip(*c)\n",
    "    \n",
    "    print(\"\\nFinished shuffling all data!\")\n",
    "    \n",
    "    #Group the same values into each bucket\n",
    "    c = list(zip(input, output))\n",
    "    values = list(set(map(lambda x:x[1], c)))\n",
    "    input_groups = [[y[0] for y in c if y[1]==x] for x in values]\n",
    "  \n",
    "\n",
    "    # Split train, test\n",
    "    trainlen = int(0.8 * data_size)\n",
    "    train_in = []\n",
    "    train_out = []\n",
    "    test_in = [] \n",
    "    test_out = []\n",
    "   \n",
    "    for i in range(0, len(values)):\n",
    "        out_label = np.zeros((1,num_class))[0]\n",
    "        out_label[values[i]] = 1\n",
    "\n",
    "        len_group = len(input_groups[i])\n",
    "        ratio_sample_int = int(max_sample / len_group)\n",
    "        ratio_sample_remain = max_sample / len_group - ratio_sample_int\n",
    "        \n",
    "        new_train_in = input_groups[i][0:int(0.8 * len_group)]\n",
    "        \n",
    "        new_train_in_up_sampling = list(np.repeat(new_train_in, ratio_sample_int, axis = 0))\n",
    "        number_addition_sample = int(ratio_sample_remain * len(new_train_in)) - len(new_train_in)\n",
    "        new_train_in_up_sampling = new_train_in_up_sampling + new_train_in[0:number_addition_sample]\n",
    "       \n",
    "        train_in  = train_in + new_train_in_up_sampling\n",
    "        \n",
    "        print(\"Up sampling sample \", i, \"in training set. \", \"The new length is: \", len(new_train_in_up_sampling))\n",
    "        new_test_in = input_groups[i][int(0.8 * len_group) : len_group]\n",
    "        test_in = test_in + new_test_in\n",
    "        train_out = train_out + [out_label] * len(new_train_in_up_sampling)\n",
    "        test_out = test_out + [out_label] * len(new_test_in)\n",
    "\n",
    "    print(\"\\nFinished upsampling data!\")\n",
    "        \n",
    "    # to numpy array    \n",
    "    train_out = np.array(train_out)\n",
    "    test_out = np.array(test_out)\n",
    "\n",
    "\n",
    "    #shuffle data\n",
    "    c = list(zip(train_in, train_out))\n",
    "    random.shuffle(c)\n",
    "    train_in, train_out = zip(*c)\n",
    "    \n",
    "    \n",
    "    print(\"\\nFinished shuffling training set\")\n",
    "    \n",
    "    \n",
    "    data_dim = 1025 * window_size\n",
    "    timesteps = None\n",
    "    \n",
    "\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(144, return_sequences=True, input_shape=(None, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(144, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "    model.add(LSTM(144))  # return a single vector of dimension 32\n",
    "#     model.add(Reshape((12,12,1), input_shape=(144, )))\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
    "#                  activation='relu',\n",
    "#                  input_shape=(1,12,12,1)))\n",
    "   \n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(6))#fully connected\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    def accuracy(model, input, output):\n",
    "        sumTrue = 0\n",
    "        for x,y in zip(input, output):\n",
    "            p = model.predict_classes(np.array([x]))\n",
    "            if (p[0] == np.argmax(y)):\n",
    "                sumTrue = sumTrue + 1\n",
    "        \n",
    "        return sumTrue / len(input)\n",
    "            \n",
    "    \n",
    "    try:\n",
    "        step = 0\n",
    "        for i in range (0,4):\n",
    "            for seq, label in zip(train_in, train_out):\n",
    "                step = step + 1\n",
    "                weight_sample = class_weight[np.argmax(label)]\n",
    "                #sample_weight = np.array([weight_sample])\n",
    "\n",
    "                model.train_on_batch(np.array([seq]), np.array([label]))\n",
    "                if (step == 1 or step % 500 == 0):\n",
    "\n",
    "                    print(\"Trained 500 sample!\")\n",
    "                    print(\"Score train set: \", accuracy(model, train_in, train_out))\n",
    "                    print(\"Score test set; \", accuracy(model, test_in, test_out))\n",
    "    except Exception as e:\n",
    "        print(\"This is error: \", e)\n",
    "\n",
    "    \n",
    "    print(\"\\nAfter training........................................................\")\n",
    "    print(\"Score train set: \", accuracy(model, train_in, train_out))\n",
    "    print(\"Score test set; \", accuracy(model, test_in, test_out))\n",
    "    \n",
    "\n",
    "    ps = []\n",
    "    for x, y in zip (test_in, test_out):\n",
    "        p = model.predict_classes(np.array([x]))\n",
    "#         print(p)\n",
    "        ps.append(p)\n",
    "    r = np.argmax(test_out, axis=1)\n",
    "    #p = model.predict_classes(test_in)\n",
    "   \n",
    "    matrix = confusion_matrix(r, ps)\n",
    "    print(\"\\nConfusion matrix: \\n\", matrix)\n",
    "    sum_colum = np.sum(matrix, axis = 0)\n",
    "   # print(\"\\nsum_column:\", sum_colum)\n",
    "    sum_row = np.sum(matrix, axis = 1)\n",
    "    #print(\"\\nsum_row:\", sum_row)\n",
    "    TP = [matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"\\nTP: \", TP,\"\\n\")   \n",
    "    FP = [sum_colum[i] - matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"FP: \", FP,\"\\n\")\n",
    "    FN = [sum_row[i] - matrix[i,i] for i in range(0, len(matrix))]\n",
    "    print(\"FN: \", FN,\"\\n\")\n",
    "    Presision = [TP[i] /(TP[i] + FP[i])  for i in range(0, len(matrix))]\n",
    "    Recall = [TP[i] /(TP[i] + FN[i])  for i in range(0, len(matrix))]\n",
    "    F1_score = [2 * Presision[i] * Recall[i] /(Presision[i] + Recall[i])  for i in range(0, len(matrix))]\n",
    "    \n",
    "  \n",
    "    \n",
    "    print(\"\\nPrecision: \", Presision,\"\\n\")\n",
    "    print(\"Recall: \", Recall,\"\\n\")\n",
    "    print(\"F1_scrore: \", F1_score, \"\\n\")\n",
    "    \n",
    "        \n",
    "    #Your statements here\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"Time for training and testing: \", stop - start, \"(s)\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
