{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import functools\n",
    "# import sets\n",
    "# import tensorflow as tf\n",
    "\n",
    "# output = [1,2,3]\n",
    "# temp = np.array(output)\n",
    "# output = np.zeros((len(output), 4))\n",
    "# output[np.arange(len(output)), temp] = 1\n",
    "# print(output)\n",
    "\n",
    "# a = [[[1,2],[3,4]], [[5,6],[7,8]]]\n",
    "\n",
    "# print(a)\n",
    "# input = [list(map(list, zip(*i))) for i in a]\n",
    "# print(input)\n",
    "# import sets\n",
    "# def get_dataset():\n",
    "#     \"\"\"Read dataset and flatten images.\"\"\"\n",
    "#     dataset = sets.Ocr()\n",
    "#     dataset = sets.OneHot(dataset.target, depth=2)(dataset, columns=['target'])\n",
    "#     dataset['data'] = dataset.data.reshape(\n",
    "#         dataset.data.shape[:-2] + (-1,)).astype(float)\n",
    "#     train, test = sets.Split(0.66)(dataset)\n",
    "#     return train, test\n",
    "\n",
    "# a, b = get_dataset()\n",
    "# batch = a.sample(10)\n",
    "# print(batch.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import basename\n",
    "import audiosegment\n",
    "from multiprocessing import Pool\n",
    "modulePath = 'ChristiansPythonLibrary/src' \n",
    "import sys\n",
    "import numpy\n",
    "sys.path.append(modulePath)\n",
    "import generalUtility\n",
    "import dspUtil\n",
    "import matplotlibUtil\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Constant\n",
    "EMOTION_ANNOTATORS = {'anger': 0, 'happiness' : 1, 'sadness' : 2, 'neutral' : 3, 'frustration' : 4, 'excited': 5,\n",
    "           'fear' : 6,'surprise' : 7,'disgust' : 8, 'other' : 9}\n",
    "\n",
    "EMOTION = {'ang': 0, 'hap' : 1, 'sad' : 2, 'neu' : 3, 'fru' : 4, 'exc': 5,\n",
    "           'fea' : 6,'sur' : 7,'dis' : 8, 'oth' : 9, 'xxx':10}\n",
    "\n",
    "METHOD = {'audio_feature':0, 'LSTM':1}\n",
    "\n",
    "#Method for classification\n",
    "method = METHOD['LSTM']\n",
    "\n",
    "#If data is processed and saved into files, just reload, dont need to re-process\n",
    "isRawDataProcessed = True\n",
    "\n",
    "#Development mode. Only run with small data.\n",
    "dev = True\n",
    "\n",
    "\n",
    "\n",
    "#Define class\n",
    "class Input:\n",
    "    ##spectral, prosody, erergy are dict type\n",
    "    def __init__(self, spectral, prosody, energy, spectrogram):\n",
    "        self.spectral = spectral\n",
    "        self.prosody = prosody\n",
    "        self.energy = energy\n",
    "        self.spectrogram = spectrogram\n",
    "        \n",
    "    def print(self):\n",
    "        print(\"spectral  features: \", spectral)\n",
    "        print(\"prosody features: \", prosody)\n",
    "        print(\"energy: \", energy)\n",
    "        print(\"spectrogram: \", spectrogram)\n",
    "        \n",
    "    def input2Vec(self, onlySpectrogram):\n",
    "        if (onlySpectrogram ==  False):\n",
    "            features = []\n",
    "            s = list(self.spectral.values())\n",
    "            p = list(self.prosody.values())\n",
    "            e = list(self.energy.values())\n",
    "            [features.extend(x) for x in [s, p, e]]\n",
    "            return features\n",
    "        else :\n",
    "            return self.spectrogram\n",
    "    \n",
    "class Output:\n",
    "    def __init__(self, duration, code, category_origin, category_evaluation, attribute):\n",
    "        self.duration = duration\n",
    "        self.code = code\n",
    "        self.category_origin = category_origin\n",
    "        self.category_evaluation = category_evaluation\n",
    "        self.attribute = attribute\n",
    "        \n",
    "     \n",
    "    def print(self):\n",
    "        print(\"duration: \", self.duration)\n",
    "        print(\"code: \", self.code)\n",
    "        print(\"category_origin: \", self.category_origin)\n",
    "        print(\"category_evaluation: \", self.category_evaluation)\n",
    "        print(\"attribute: \", self.attribute)\n",
    "        \n",
    "    def output2Vec(self):\n",
    "        emotion = EMOTION[self.category_origin]\n",
    "        return emotion\n",
    "    \n",
    "    \n",
    "    \n",
    "#Functions for get features from audio file\n",
    "def amp2Db(samples):\n",
    "    dbs = []\n",
    "    for  x in samples:\n",
    "        if x < 0:\n",
    "            v = - dspUtil.rmsToDb(np.abs(x))\n",
    "        elif x == 0:\n",
    "            v = 0\n",
    "        else :\n",
    "            v = dspUtil.rmsToDb(np.abs(x))\n",
    "        dbs.append(v)\n",
    "    return dbs\n",
    "\n",
    "def getF0Features(file):\n",
    "    features = {}\n",
    "    sound = audiosegment.from_file(file)\n",
    "    voiced = sound.filter_silence(duration_s=0.2)\n",
    "    frame_rate = sound.frame_rate\n",
    "    frames = sound.dice(0.032)\n",
    "\n",
    "    f0s = []\n",
    "    for f in frames:\n",
    "        f0 = dspUtil.calculateF0once(amp2Db(f.get_array_of_samples()), frame_rate)\n",
    "        if(f0 != 0):\n",
    "            f0s.append(f0)\n",
    "    \n",
    "    features['f0_min'] = np.min(f0s)\n",
    "    features['f0_max'] = np.max(f0s)\n",
    "    features['f0_range'] = np.max(f0s) - np.min(f0s)\n",
    "    features['f0_mean'] = np.mean(f0s)\n",
    "    features['f0_median'] = np.median(f0s)\n",
    "    features['f0_25th'] = np.percentile(f0s, 25)\n",
    "    features['f0_75th'] = np.percentile(f0s, 75)\n",
    "    features['f0_std'] = np.std(f0s)\n",
    "    \n",
    "  \n",
    "    return features\n",
    "\n",
    "def getEnergyFeatures(file):\n",
    "    features = {}\n",
    "    sound = audiosegment.from_file(file)\n",
    "    voiced = sound.filter_silence(duration_s=0.2)\n",
    "    samples = voiced.get_array_of_samples()\n",
    "    frame_rate = sound.frame_rate\n",
    "    frames = sound.dice(0.032)\n",
    "    \n",
    "    e = []\n",
    "    for f in frames:\n",
    "        e.append(np.abs(f.max_dBFS))\n",
    "    \n",
    "    \n",
    "    features['energy_min'] = np.min(e)\n",
    "    features['energy_max'] = np.max(e)\n",
    "    features['energy_range'] = np.max(e) - np.min(e)\n",
    "    features['energy_mean'] = np.mean(e)\n",
    "    features['energy_median'] = np.median(e)\n",
    "    features['energy_25th'] = np.percentile(e, 25)\n",
    "    features['energy_75th'] = np.percentile(e, 75)\n",
    "    features['energy_std'] = np.std(e)   \n",
    "\n",
    "    return features\n",
    "    \n",
    "def audio2Features(file):\n",
    "    spectral = {}\n",
    "    prosody = {}\n",
    "    energy = {}\n",
    "    try:\n",
    "        prosody = getF0Features(file)\n",
    "        energy = getEnergyFeatures(file)\n",
    "        \n",
    "        y, sr = librosa.load(file)\n",
    "        spectrogram = librosa.stft(librosa.amplitude_to_db(y))\n",
    "        spectrogram = np.abs(spectrogram)\n",
    "        #To be continued....\n",
    "    \n",
    "        return Input(spectral, prosody, energy, spectrogram)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "#Function for getting input vector and corresponding output      \n",
    "def parallel_task(d0, d1):\n",
    "    print(\"task...\")\n",
    "    # Each input diectory contains many file\n",
    "    # This fucntion will walk through all valid 'wav'files in this directory and get features like engergy, frequency...\n",
    "    def parseInput(dir):\n",
    "        dicts = {} \n",
    "        for f in os.listdir(dir):\n",
    "            if not f.startswith(\".\") and os.path.splitext(f)[1] == \".wav\":\n",
    "                dicts[os.path.splitext(f)[0]] = audio2Features(dir + \"/\" + f)\n",
    "\n",
    "\n",
    "        return dicts\n",
    "    \n",
    "    # Each output file contains label of many diffrent 'wav' file.\n",
    "    # This function will parse content of text file using 'regrex'. Then turn it into label\n",
    "    def parseOutput(file):\n",
    "        dict_namefile_output = {}\n",
    "        # Open file to get all contents excepts the first line.\n",
    "        f = open(file, 'r')\n",
    "        content = \"\"\n",
    "        index = 0\n",
    "        for line in f:\n",
    "            index = index + 1\n",
    "            if index == 1:\n",
    "                continue\n",
    "            content  = content + line\n",
    "\n",
    "        # Find all matched patterns in the content\n",
    "        ps = re.findall(r'\\[.*?\\)\\n\\n', content, re.DOTALL)\n",
    "\n",
    "        # Parse each matched pattern into  'Output' object\n",
    "        try:\n",
    "            for p in ps:\n",
    "                ls = p.split(\"\\n\")\n",
    "                ls = list(filter(lambda x: len(x) > 0 ,ls))\n",
    "\n",
    "                # Split elements of the first line which looks like : \n",
    "                # [147.0300 - 151.7101]\tSes01F_impro02_M012\tneu\t[2.5000, 2.0000, 2.0000]\n",
    "                ele_line0 = re.search(r'(\\[.*?\\])(\\s)(.*?)(\\s)(.*?)(\\s)(\\[.*?\\])', ls[0]).groups()\n",
    "\n",
    "                # Split time components which looks like:\n",
    "                # [147.0300 - 151.7101]\n",
    "                time_dur = ele_line0[0]\n",
    "                ele_time_dur = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", time_dur)\n",
    "                ele_time_dur = [float(x) for x in ele_time_dur]\n",
    "\n",
    "                # Get code and category_origin which looks like:\n",
    "                # Code: Ses01F_impro02_M012\n",
    "                # Category_origin: neu\n",
    "                code = ele_line0[2]\n",
    "                category_origin = ele_line0[4]\n",
    "\n",
    "                # Split attribute components which looks like:\n",
    "                # [2.5000, 2.0000, 2.0000]\n",
    "                attribute = ele_line0[6]\n",
    "                ele_attribute = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", attribute)\n",
    "                ele_attribute = [float(x) for x in ele_attribute]\n",
    "\n",
    "                # Get categorial_evaluation:\n",
    "                lines_categorical = list(filter(lambda x : x[0] == 'C', ls))\n",
    "                rex = re.compile(r'C.*?:(\\s)(.*?)(\\s)\\(.*?\\)')\n",
    "\n",
    "                category_evaluation = []\n",
    "                for l in lines_categorical:\n",
    "                    elements = rex.search(l).groups()\n",
    "                    cat = elements[1]\n",
    "                    cat = cat.split(\";\")\n",
    "                    cat = map(lambda x: x.lstrip(), cat)\n",
    "                    cat = list(filter(lambda x: len(x)>0, cat))\n",
    "                    category_evaluation.extend(cat)\n",
    "\n",
    "\n",
    "                # Make list distinct\n",
    "                category_evaluation = list(set(category_evaluation))\n",
    "                \n",
    "                \n",
    "\n",
    "                # Make dict {name_file : parsed_output}\n",
    "                dict_namefile_output[code] = Output(ele_time_dur, code, category_origin, category_evaluation, ele_attribute)\n",
    "            return dict_namefile_output\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    ### Parse input and output files and get input and output as vector\n",
    "    dicts_in = parseInput(d0)\n",
    "    dicts_out = parseOutput(d1)\n",
    "    in_out = []\n",
    "    \n",
    "    keys = list(dicts_in.keys())\n",
    "    for key in keys:\n",
    "        if(dicts_out[key].category_origin != 'xxx'):\n",
    "            if (method == METHOD['LSTM']):\n",
    "                in_out.append((dicts_in[key].input2Vec(onlySpectrogram=True), dicts_out[key].output2Vec()))\n",
    "            else:\n",
    "                in_out.append((dicts_in[key].input2Vec(onlySpectrogram=False), dicts_out[key].output2Vec()))\n",
    "    return in_out\n",
    "    \n",
    "    \n",
    "def createInput_Output():\n",
    "    ### Get directories of input and output\n",
    "    DATA_DIR = \"IEMOCAP_full_release\"\n",
    "    NUM_SESSION = 5\n",
    "    input_output = []\n",
    "    for i in range (1, NUM_SESSION + 1):\n",
    "        name_session = \"Session\" + str(i)\n",
    "        root_dir_of_wav = DATA_DIR + \"/\" + name_session + \"/sentences\" + \"/wav\"\n",
    "        root_dir_of_labels = DATA_DIR + \"/\" + name_session + \"/dialog\" + \"/EmoEvaluation\"\n",
    "\n",
    "        for x in os.walk(root_dir_of_wav):\n",
    "            if(x[0] == root_dir_of_wav):\n",
    "                dirs_of_wav = x[1]\n",
    "                index = -1\n",
    "            else:\n",
    "                index = index + 1\n",
    "                input_output.append((x[0], root_dir_of_labels + \"/\" + dirs_of_wav[index] + \".txt\"))\n",
    "                \n",
    "    \n",
    "    ds = input_output\n",
    "    in_out = []\n",
    "    input = []\n",
    "    out = []\n",
    "    \n",
    "    # Multi processing\n",
    "    with Pool(processes=8) as pool:\n",
    "         in_out = pool.starmap(parallel_task, ds)\n",
    "   \n",
    "    r = []\n",
    "    for e in in_out:\n",
    "        r = r + e\n",
    "    \n",
    "    input = [x[0] for x in r]\n",
    "    out = [x[1] for x in r]\n",
    "    print(\"Finished creating input output into txt file\")\n",
    "    return (input, out)\n",
    " \n",
    "\n",
    "\n",
    "#If have not processed data yet then process, otherwise loading data from file.\n",
    "if isRawDataProcessed == False:\n",
    "\n",
    "    ##Get input, normalize input, get output\n",
    "    input, output = createInput_Output()\n",
    "    output = np.array(output)\n",
    "    \n",
    "    if(method == METHOD['audio_feature']):\n",
    "        input = np.array(input)\n",
    "        input = input / input.max(axis=0)\n",
    "        filehandlerInput = open('input0.obj', 'wb')\n",
    "        filehandlerOutput = open('output0.obj', 'wb')\n",
    "    elif(method == METHOD['LSTM']):\n",
    "        # After this operator, each sample will be a 2-D array, Each row includes magnitude energy values in range of frquencies\n",
    "        # Rows will have the same length in all samples.\n",
    "        # Each sample will have different number of rows beacause their difference of length in seconds \n",
    "        #input = [list(map(list, zip(*i))) for i in input]\n",
    "        filehandlerInput = open('input1.obj', 'wb')\n",
    "        filehandlerOutput = open('output1.obj', 'wb')\n",
    "\n",
    "        \n",
    "    pickle.dump(input, filehandlerInput)\n",
    "    pickle.dump(output, filehandlerOutput)\n",
    "    print(\"Finish write processed data (input, output) to file!!!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c99b13702131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#### Training using LSTM and CNN\n",
    "\n",
    "\n",
    "# Example for my blog post at:\n",
    "# https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "import functools\n",
    "import sets\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceLabelling:\n",
    "\n",
    "    def __init__(self, data, target, num_hidden=200, num_layers=3):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # Recurrent network.\n",
    "        output, _ = rnn.dynamic_rnn(\n",
    "            rnn_cell.GRUCell(self._num_hidden),\n",
    "            self.data,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.length,\n",
    "        )\n",
    "        # Softmax layer.\n",
    "        max_length = int(self.target.get_shape()[1])\n",
    "        num_classes = int(self.target.get_shape()[2])\n",
    "        weight, bias = self._weight_and_bias(self._num_hidden, num_classes)\n",
    "        # Flatten to apply same weights to all time steps.\n",
    "        output = tf.reshape(output, [-1, self._num_hidden])\n",
    "        prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)\n",
    "        prediction = tf.reshape(prediction, [-1, max_length, num_classes])\n",
    "        return prediction\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        # Compute cross entropy for each frame.\n",
    "        cross_entropy = self.target * tf.log(self.prediction)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.target), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "        # Average over actual sequence lengths.\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32)\n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.0003\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 2), tf.argmax(self.prediction, 2))\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.target), reduction_indices=2))\n",
    "        mistakes *= mask\n",
    "        # Average over actual sequence lengths.\n",
    "        mistakes = tf.reduce_sum(mistakes, reduction_indices=1)\n",
    "        mistakes /= tf.cast(self.length, tf.float32)\n",
    "        return tf.reduce_mean(mistakes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "##Running\n",
    "if (method == METHOD['LSTM']):\n",
    "    \n",
    "    ##Loading  data from files\n",
    "    filehandlerInput = open('input1.obj', 'rb')\n",
    "    filehandlerOutput = open('output1.obj', 'rb')\n",
    "    input = pickle.load(filehandlerInput)\n",
    "    output = pickle.load(filehandlerOutput)\n",
    "    \n",
    "    #if in development mode, just use the small data!\n",
    "    if (dev):\n",
    "        data_size = 10\n",
    "    else:\n",
    "        data_size = len(input)\n",
    "    \n",
    "    input = [list(map(list, zip(*i))) for i in input[0:data_size]]\n",
    "    output = output[0:data_size]\n",
    "    \n",
    "    #One-hot encoding output\n",
    "    temp = np.array(output)\n",
    "    output = np.zeros((len(output), 10))\n",
    "    output[np.arange(len(output)), temp] = 1\n",
    "    \n",
    "    # Split train, test\n",
    "    train_in, train_out = input[0:7], output[0:7]\n",
    "    test_in, test_out = input[8:data_size], output[8:data_size]\n",
    "    print(\"Finished split data!\")\n",
    "    \n",
    "    \n",
    "    num_classes = 10\n",
    "    row_size = len(input[0][0])\n",
    "    data = tf.placeholder(tf.float32, [None, None, row_size])\n",
    "    target = tf.placeholder(tf.float32, [None, None, num_classes])\n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "    model = VariableSequenceLabelling(data, target, dropout)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        for i in range(len(train_in)):\n",
    "            print(\"train.....\")\n",
    "            sess.run(model.optimize, {\n",
    "                data: train_in, target: train_out, dropout: 0.5})\n",
    "        error = sess.run(model.error, {\n",
    "            data: test_in, target: test_out, dropout: 1})\n",
    "        print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of training set:  0.04487179487179487\n",
      "Score of validation set:  0.06896551724137931\n",
      "Score of training set:  0.0\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.0\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.001282051282051282\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.001282051282051282\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.0\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.0\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.0012804097311139564\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.0\n",
      "Score of validation set:  0.0\n",
      "Score of training set:  0.0\n",
      "Score of validation set:  0.0\n",
      "Average accuracy training set, std: 0.0048716307167011395   0.013345699219885505\n",
      "Average accuracy validation set, std: 0.006896551724137931   0.02068965517241379\n",
      "dict_keys(['beta_1', 'tol', 'solver', 'random_state', 'nesterovs_momentum', 'beta_2', 'early_stopping', 'learning_rate_init', 'warm_start', 'alpha', 'batch_size', 'validation_fraction', 'epsilon', 'shuffle', 'momentum', 'learning_rate', 'activation', 'verbose', 'max_iter', 'power_t', 'hidden_layer_sizes'])\n",
      "predicts:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "prob:  [1.36095115e-01 7.61648402e-02 1.32606196e-01 3.41447317e-01\n",
      " 2.12288883e-01 8.19692728e-02 5.68860433e-03 1.14564520e-02\n",
      " 1.16879622e-03 1.17610659e-03 3.55825432e-06]\n",
      "SCore for test set:  0.0\n"
     ]
    }
   ],
   "source": [
    "#### Training based on features of audio\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def training(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    kf = KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "    i_fold = 0\n",
    "    accuracy_train_results = []\n",
    "    accuracy_valid_results = []\n",
    "\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        i_fold = i_fold + 1\n",
    "        \n",
    "        x_train_sub, x_valid_sub = X_train[train_index], X_train[valid_index]\n",
    "        y_train_sub, y_valid_sub = y_train[train_index], y_train[valid_index]\n",
    "        clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20,), random_state=1)\n",
    "        clf.fit(x_train_sub, y_train_sub)\n",
    "        \n",
    "        score = clf.score(x_train_sub, y_train_sub)\n",
    "        score1 = clf.score(x_valid_sub, y_valid_sub)\n",
    "        accuracy_train_results.append(score)\n",
    "        accuracy_valid_results.append(score1)\n",
    "        \n",
    "        print(\"Score of training set: \", score)\n",
    "        print(\"Score of validation set: \", score1)\n",
    "     \n",
    "       \n",
    "    \n",
    "    avg_accuracy_train_result = np.sum(accuracy_train_results) / len(accuracy_train_results)\n",
    "    avg_accuracy_valid_result = np.sum(accuracy_valid_results) / len(accuracy_valid_results)\n",
    "    print(\"Average accuracy training set, std:\", avg_accuracy_train_result, \" \",\\\n",
    "          np.std(accuracy_train_results))\n",
    "    print(\"Average accuracy validation set, std:\", avg_accuracy_valid_result,\" \", \\\n",
    "          np.std(accuracy_valid_results))     \n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "\n",
    "    predicts = clf.predict(X_test)\n",
    "    pro = clf.predict_proba(X_test)\n",
    " #   print(\"predicts: \", predicts)\n",
    " #   print(\"prob: \", pro[0])\n",
    "    score_test = clf.score(X_test, y_test)\n",
    "    print(\"\\nScore for test set: \", score_test)\n",
    "    print (\"\\nConfusion matrix:..................... \")\n",
    "    matrix = confusion_matrix(y_test, predicts)\n",
    "    matrix_ratio = matrix/matrix.sum(1, keepdims=True)\n",
    "    print(matrix)\n",
    "    print(\"\\n\", \"Confusion matrix ratio:\")\n",
    "    print(matrix_ratio)\n",
    "    print(\"\\n\", \"Horizontal of confusion matrix ratio:\")\n",
    "    hor = [matrix_ratio[i,i] for i in range(0, len(matrix_ratio))]\n",
    "    print(hor)\n",
    " \n",
    "\n",
    "if (method == METHOD['audio_feature']):\n",
    "\n",
    "    ##Loading  data from files\n",
    "    filehandlerInput = open('input0.obj', 'rb')\n",
    "    filehandlerOutput = open('output0.obj', 'rb')\n",
    "    input = pickle.load(filehandlerInput)\n",
    "    output = pickle.load(filehandlerOutput)\n",
    "\n",
    "    # Get quantiry of each label\n",
    "    y = np.bincount(output)\n",
    "    ii = np.nonzero(y)[0]\n",
    "    a = list(zip(ii,y[ii]))\n",
    "    print(\"EMOTION_ANNOTATE: \", EMOTION_ANNOTATORS)\n",
    "    print(\"\\nThe quantity of each label: \", a, \"\\n\")\n",
    "\n",
    "    #Remove labels that have small quantity.\n",
    "    indices = [] \n",
    "    for i in range(0, len(output)):\n",
    "        if output[i] >= 6:\n",
    "            indices.append(i)\n",
    "    input = np.delete(input, indices, axis = 0)\n",
    "    output = np.delete(output, indices)\n",
    "\n",
    "    #Training and testing\n",
    "    training(input, output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 6 2 2 2]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
